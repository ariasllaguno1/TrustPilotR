# TrustPilot Scraper - GitHub Actions

## ğŸ¤– Ejecutar el scraper automÃ¡ticamente en GitHub

Este proyecto permite hacer scraping de reseÃ±as de TrustPilot (categorÃ­a Viajes y Vacaciones) directamente desde GitHub Actions, sin necesidad de ejecutarlo en tu laptop.

## ğŸš€ CÃ³mo usar

### 1. ConfiguraciÃ³n inicial (solo la primera vez)

1. **Fork este repositorio** o clÃ³nalo en tu cuenta de GitHub
2. Ve a la pestaÃ±a **"Actions"** de tu repositorio
3. Si es la primera vez, acepta habilitar GitHub Actions

### 2. Ejecutar el scraper

1. **Ve a la pestaÃ±a "Actions"** de tu repositorio
2. **Selecciona "Ejecutar TrustPilotScraper"** en la lista de workflows
3. **Haz clic en "Run workflow"** (botÃ³n verde a la derecha)
4. **Configura los parÃ¡metros:**
   - **Empresas mÃ¡ximas**: CuÃ¡ntas empresas procesar (por defecto: 100)
   - **PÃ¡ginas de reseÃ±as por empresa**: CuÃ¡ntas pÃ¡ginas de reseÃ±as extraer de cada empresa (por defecto: 10)
   - **PÃ¡ginas de categorÃ­a**: CuÃ¡ntas pÃ¡ginas de la categorÃ­a recorrer para encontrar empresas (por defecto: 10)
5. **Haz clic en "Run workflow"**

### 3. Monitorear el progreso

- El workflow aparecerÃ¡ en la lista con un Ã­cono amarillo â­• (ejecutÃ¡ndose) o verde âœ… (completado)
- Haz clic en la ejecuciÃ³n para ver el progreso en tiempo real
- El proceso puede tomar desde 30 minutos hasta varias horas dependiendo de los parÃ¡metros

### 4. Descargar los resultados

Una vez completado el workflow:

1. **Ve a la ejecuciÃ³n completada** en la pestaÃ±a Actions
2. **Busca la secciÃ³n "Artifacts"** al final de la pÃ¡gina
3. **Descarga los archivos:**
   - `csv-files`: Todos los archivos CSV con las reseÃ±as
   - `checkpoints`: Archivos de progreso (por si quieres reanudar)
   - `executed-notebook`: El notebook ejecutado con resultados
   - `error-logs`: Logs de errores si los hubo

## ğŸ“Š QuÃ© contienen los archivos

### Archivos CSV principales:
- `trustpilot_consolidated_YYYYMMDD_HHMMSS.csv`: **Archivo principal** con todas las reseÃ±as
- `companies_processed_YYYYMMDD_HHMMSS.csv`: Lista de empresas procesadas
- `results/reviews_[dominio]_YYYYMMDD_HHMMSS.csv`: ReseÃ±as individuales por empresa

### Columnas del dataset:
- **InformaciÃ³n bÃ¡sica**: `review_id`, `domain`, `company_name`, `categories`, `subcategories`
- **Detalles de la reseÃ±a**: `review_date`, `customer_name`, `customer_score`, `review_text`
- **Para anÃ¡lisis posterior**: `language`, `sentiment`, `emotion`, `customer_gender`, `main_topic`, `keywords`, etc.

## âš™ï¸ ConfiguraciÃ³n avanzada

### LÃ­mites recomendados:

| Uso | Empresas | PÃ¡ginas/empresa | PÃ¡ginas categorÃ­a | Tiempo aprox. |
|-----|----------|-----------------|-------------------|---------------|
| **Prueba rÃ¡pida** | 10 | 3 | 3 | 10-15 min |
| **Dataset pequeÃ±o** | 50 | 10 | 10 | 1-2 horas |
| **Dataset mediano** | 200 | 20 | 20 | 3-4 horas |
| **Dataset grande** | 500+ | 50+ | 50+ | 5-6 horas |

### LÃ­mites de GitHub Actions:
- **Tiempo mÃ¡ximo**: 6 horas por ejecuciÃ³n
- **Almacenamiento**: Los artefactos se conservan 30 dÃ­as
- **Concurrencia**: Solo una ejecuciÃ³n simultÃ¡nea

## ğŸ”§ Archivos del proyecto

- `.github/workflows/scraper.yml`: ConfiguraciÃ³n del workflow de GitHub Actions
- `scraper_github_actions.py`: Scraper optimizado para ejecutar en CI/CD
- `TrustPilotScraper.ipynb`: Notebook original para ejecutar localmente
- `requirements.txt`: Dependencias de Python

## âš ï¸ Consideraciones importantes

### TÃ©rminos de uso:
- **Respeta los tÃ©rminos de servicio** de TrustPilot
- **Uso acadÃ©mico/investigaciÃ³n**: Este scraper estÃ¡ diseÃ±ado para fines educativos
- **Rate limiting**: El scraper incluye delays aleatorios para ser respetuoso

### Limitaciones tÃ©cnicas:
- **Modo headless**: Se ejecuta sin interfaz grÃ¡fica en servidores de GitHub
- **Sin persistencia**: Los checkpoints se pierden entre ejecuciones (solo Ãºtiles si el workflow falla)
- **DetecciÃ³n anti-bot**: TrustPilot puede bloquear solicitudes automatizadas

## ğŸ†˜ SoluciÃ³n de problemas

### El workflow falla:
1. **Revisa los logs** en la pestaÃ±a Actions
2. **Reduce los parÃ¡metros** (menos empresas/pÃ¡ginas)
3. **Espera un tiempo** antes de volver a ejecutar

### No se generan archivos CSV:
- Posible bloqueo de TrustPilot
- Cambios en la estructura de la pÃ¡gina
- Revisa los logs de errores en los artefactos

### El workflow se detiene por timeout:
- Reduce el nÃºmero de empresas o pÃ¡ginas
- GitHub Actions tiene un lÃ­mite de 6 horas

## ğŸ¯ PrÃ³ximos pasos

Una vez que tengas los datos:

1. **AnÃ¡lisis bÃ¡sico**: Usa pandas para explorar los datos
2. **AnÃ¡lisis de sentimientos**: Usa las columnas preparadas para LLM
3. **Visualizaciones**: Crea grÃ¡ficos con matplotlib/seaborn
4. **Machine Learning**: Entrena modelos para clasificaciÃ³n de reseÃ±as

## ğŸ“ Ejemplo de uso en Python

```python
import pandas as pd

# Cargar los datos descargados
df = pd.read_csv('trustpilot_consolidated_YYYYMMDD_HHMMSS.csv')

# ExploraciÃ³n bÃ¡sica
print(f"Total de reseÃ±as: {len(df)}")
print(f"Empresas Ãºnicas: {df['company_name'].nunique()}")
print(f"DistribuciÃ³n de puntuaciones:")
print(df['customer_score'].value_counts().sort_index())

# Filtrar reseÃ±as por puntuaciÃ³n
reseÃ±as_positivas = df[df['customer_score'] >= 4]
reseÃ±as_negativas = df[df['customer_score'] <= 2]

# AnÃ¡lisis por empresa
empresas_top = df.groupby('company_name').agg({
    'customer_score': 'mean',
    'review_id': 'count'
}).rename(columns={'review_id': 'num_reviews'}).sort_values('num_reviews', ascending=False)

print(empresas_top.head())
```

---

## ğŸ“§ Soporte

Si tienes problemas o preguntas, revisa:
1. Los logs del workflow en GitHub Actions
2. Los archivos de error en los artefactos
3. La documentaciÃ³n de los notebooks originales 