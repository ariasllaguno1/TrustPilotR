{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Scraper de Trustpilot (Categor√≠a: Viajes y Vacaciones)\n",
    "\n",
    "Este notebook extrae informaci√≥n de empresas y rese√±as de https://es.trustpilot.com/categories/travel_vacation usando Selenium.\n",
    "\n",
    "## Variables que extrae:\n",
    "- ID de la rese√±a, Dominio, Nombre de la empresa, Categor√≠as, Subcategor√≠as, Calificaci√≥n de la empresa\n",
    "- Fecha de la rese√±a, Nombre del cliente, Puntuaci√≥n del cliente, Texto de la rese√±a\n",
    "- Columnas vac√≠as para LLM: Idioma, Sentimiento, Emoci√≥n, G√©nero del cliente, Tema principal, Palabras clave, Tipo de cliente, Tipo de turista, Tipo de grupo, ¬øAnalizado?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del driver de Chrome\n",
    "def setup_driver(headless=False):\n",
    "    \"\"\"Configura y retorna el driver de Chrome con las opciones necesarias\"\"\"\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    # Modo headless opcional\n",
    "    if headless:\n",
    "        chrome_options.add_argument('--headless')\n",
    "    \n",
    "    # Opciones para evitar detecci√≥n\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    # Desactivar im√°genes para cargar m√°s r√°pido (opcional)\n",
    "    # prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    # chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    # User agent realista\n",
    "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36')\n",
    "    \n",
    "    # Tama√±o de ventana\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    \n",
    "    # Otras opciones para parecer m√°s humano\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    chrome_options.add_argument('--proxy-server=\"direct://\"')\n",
    "    chrome_options.add_argument('--proxy-bypass-list=*')\n",
    "    chrome_options.add_argument('--start-maximized')\n",
    "    \n",
    "    try:\n",
    "        # Inicializar driver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        # Ejecutar JavaScript para ocultar webdriver\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        \n",
    "        driver.implicitly_wait(10)\n",
    "        print(\"‚úÖ Driver Chrome iniciado correctamente\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al iniciar Chrome: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para delays aleatorios m√°s humanos\n",
    "import random\n",
    "\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    \"\"\"Pausa aleatoria para parecer m√°s humano\"\"\"\n",
    "    delay = random.uniform(min_seconds, max_seconds)\n",
    "    time.sleep(delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "def generate_review_id(company_name, review_date, customer_name, review_text):\n",
    "    \"\"\"Genera un ID √∫nico para cada rese√±a\"\"\"\n",
    "    content = f\"{company_name}{review_date}{customer_name}{review_text[:50]}\"\n",
    "    return hashlib.md5(content.encode()).hexdigest()[:12]\n",
    "\n",
    "def parse_date(date_str):\n",
    "    \"\"\"Convierte la fecha del formato de Trustpilot a formato est√°ndar\"\"\"\n",
    "    try:\n",
    "        # Mapeo de meses en espa√±ol\n",
    "        meses = {\n",
    "            'enero': 'January', 'febrero': 'February', 'marzo': 'March',\n",
    "            'abril': 'April', 'mayo': 'May', 'junio': 'June',\n",
    "            'julio': 'July', 'agosto': 'August', 'septiembre': 'September',\n",
    "            'octubre': 'October', 'noviembre': 'November', 'diciembre': 'December'\n",
    "        }\n",
    "        \n",
    "        # Reemplazar mes espa√±ol por ingl√©s\n",
    "        for mes_esp, mes_eng in meses.items():\n",
    "            date_str = date_str.replace(mes_esp, mes_eng)\n",
    "        \n",
    "        # Parsear fecha\n",
    "        return pd.to_datetime(date_str, format='%d de %B de %Y')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def scroll_to_load_reviews(driver, max_scrolls=10):\n",
    "    \"\"\"Hace scroll para cargar m√°s rese√±as\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scrolls = 0\n",
    "    \n",
    "    while scrolls < max_scrolls:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "            \n",
    "        last_height = new_height\n",
    "        scrolls += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de debugging para analizar la estructura de la p√°gina\n",
    "def debug_page_structure(driver, url):\n",
    "    \"\"\"Analiza la estructura de la p√°gina para encontrar selectores correctos\"\"\"\n",
    "    print(f\"\\nüîç DEBUGGING: Analizando estructura de {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Buscar todos los enlaces que podr√≠an ser de empresas\n",
    "    review_links = soup.find_all('a', href=re.compile('/review/'))\n",
    "    print(f\"üìå Enlaces con /review/: {len(review_links)}\")\n",
    "    if review_links:\n",
    "        print(\"   Primeros 3 ejemplos:\")\n",
    "        for i, link in enumerate(review_links[:3]):\n",
    "            print(f\"   - {link.get('href')} | Texto: {link.text.strip()[:50]}\")\n",
    "    \n",
    "    # Buscar elementos article\n",
    "    articles = soup.find_all('article')\n",
    "    print(f\"üìå Elementos <article>: {len(articles)}\")\n",
    "    \n",
    "    # Buscar elementos con clases que contengan 'business' o 'company'\n",
    "    business_elements = soup.find_all(class_=re.compile('business|company', re.I))\n",
    "    print(f\"üìå Elementos con clases 'business/company': {len(business_elements)}\")\n",
    "    \n",
    "    # Buscar elementos con data attributes\n",
    "    data_attrs = soup.find_all(attrs={'data-business-unit-card': True})\n",
    "    print(f\"üìå Elementos con data-business-unit-card: {len(data_attrs)}\")\n",
    "    \n",
    "    # Guardar HTML para an√°lisis manual\n",
    "    with open('trustpilot_debug.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(str(soup.prettify()))\n",
    "    print(\"üíæ HTML guardado en trustpilot_debug.html\")\n",
    "    \n",
    "    # Tomar screenshot\n",
    "    driver.save_screenshot('trustpilot_debug.png')\n",
    "    print(\"üì∏ Screenshot guardado en trustpilot_debug.png\")\n",
    "    \n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para extraer informaci√≥n de las empresas\n",
    "def get_companies_from_category(driver, category_url, max_pages=75):\n",
    "    \"\"\"Extrae informaci√≥n de empresas de una categor√≠a\"\"\"\n",
    "    companies = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{category_url}?page={page}\"\n",
    "        print(f\"üîç Accediendo a: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Aumentar tiempo de espera\n",
    "        \n",
    "        # Guardar HTML para debugging\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Diferentes selectores posibles para las tarjetas de empresas\n",
    "        # ORDEN MODIFICADO: El selector 'paper_paper__' causaba duplicados (encontraba los mismos 10 elementos en cada p√°gina)\n",
    "        selectors = [\n",
    "            {'tag': 'div', 'attrs': {'data-business-unit-card': True}},\n",
    "            {'tag': 'article', 'attrs': {'class': re.compile('styles_businessUnitCard')}},\n",
    "            {'tag': 'div', 'attrs': {'class': re.compile('styles_businessUnitCard')}},\n",
    "            # Usar directamente los enlaces que S√ç funcionan (debug muestra ~30 por p√°gina)\n",
    "            {'tag': 'a', 'attrs': {'href': re.compile('/review/[^/?]+$')}},  # Solo enlaces que terminan con dominio\n",
    "            # ELIMINADO: {'tag': 'div', 'attrs': {'class': re.compile('paper_paper__')}}, # ¬°Este causaba los duplicados!\n",
    "        ]\n",
    "        \n",
    "        company_cards = []\n",
    "        for selector in selectors:\n",
    "            company_cards = soup.find_all(selector['tag'], attrs=selector['attrs'])\n",
    "            if company_cards:\n",
    "                print(f\"‚úÖ Encontradas {len(company_cards)} tarjetas usando selector: {selector}\")\n",
    "                break\n",
    "        \n",
    "        if not company_cards:\n",
    "            print(f\"‚ö†Ô∏è No se encontraron tarjetas de empresas en la p√°gina {page}\")\n",
    "            print(\"üîç Guardando screenshot para debugging...\")\n",
    "            driver.save_screenshot(f\"trustpilot_page_{page}_debug.png\")\n",
    "            \n",
    "            # Buscar enlaces alternativos\n",
    "            all_links = soup.find_all('a', href=re.compile('/review/'))\n",
    "            if all_links:\n",
    "                print(f\"üìå Encontrados {len(all_links)} enlaces de review\")\n",
    "            \n",
    "            # Si es la primera p√°gina, intentar con wait m√°s espec√≠fico\n",
    "            if page == 1:\n",
    "                try:\n",
    "                    print(\"‚è≥ Esperando carga completa de la p√°gina...\")\n",
    "                    WebDriverWait(driver, 15).until(\n",
    "                        EC.presence_of_element_located((By.TAG_NAME, \"article\"))\n",
    "                    )\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    company_cards = soup.find_all('article')\n",
    "                    if company_cards:\n",
    "                        print(f\"‚úÖ Encontrados {len(company_cards)} articles despu√©s de esperar\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if not company_cards:\n",
    "                continue\n",
    "        \n",
    "        # Si son enlaces directos, procesarlos de manera diferente\n",
    "        if company_cards and company_cards[0].name == 'a':\n",
    "            for link in company_cards:\n",
    "                try:\n",
    "                    company_url = link.get('href', '')\n",
    "                    if '/review/' not in company_url:\n",
    "                        continue\n",
    "                        \n",
    "                    if not company_url.startswith('http'):\n",
    "                        company_url = f\"https://es.trustpilot.com{company_url}\"\n",
    "                    \n",
    "                    # Obtener dominio primero para limpiar el nombre\n",
    "                    domain = company_url.split('/')[-1].split('?')[0]  # Remover par√°metros\n",
    "                    \n",
    "                    # Obtener el nombre de la empresa con mejores estrategias\n",
    "                    company_name = \"\"\n",
    "                    \n",
    "                    # Estrategia 1: Buscar en el texto del enlace y limpiar\n",
    "                    link_text = link.get_text(separator=' ', strip=True)\n",
    "                    if link_text:\n",
    "                        # Limpiar texto com√∫n no deseado\n",
    "                        company_name = link_text.replace('M√°s relevantes', '').replace('M√°s relevante', '').strip()\n",
    "                        \n",
    "                        # Si el dominio est√° en el texto, usar lo que est√° antes\n",
    "                        if domain in company_name:\n",
    "                            parts = company_name.split(domain)\n",
    "                            if parts and parts[0]:\n",
    "                                company_name = parts[0].strip()\n",
    "                        \n",
    "                        # Si hay n√∫meros (rating), cortar antes del primer n√∫mero\n",
    "                        match = re.search(r'^([^0-9]+?)(?:\\d|$)', company_name)\n",
    "                        if match:\n",
    "                            company_name = match.group(1).strip()\n",
    "                    \n",
    "                    # Estrategia 2: Buscar en elementos hijos si no encontramos nombre\n",
    "                    if not company_name or len(company_name) < 2:\n",
    "                        name_elem = link.find(['span', 'p', 'h2', 'h3'])\n",
    "                        if name_elem:\n",
    "                            company_name = name_elem.text.strip()\n",
    "                    \n",
    "                    # Fallback: usar el dominio limpio\n",
    "                    if not company_name or len(company_name) < 2:\n",
    "                        company_name = domain.replace('.com', '').replace('.es', '').replace('-', ' ').title()\n",
    "                    \n",
    "                    # Buscar informaci√≥n adicional en el contenedor padre\n",
    "                    parent = link.parent\n",
    "                    rating = \"N/A\"\n",
    "                    num_reviews = \"0\"\n",
    "                    \n",
    "                    if parent:\n",
    "                        # Buscar calificaci√≥n\n",
    "                        rating_elem = parent.find(text=re.compile(r'\\d+[,\\.]\\d+'))\n",
    "                        if rating_elem:\n",
    "                            rating = re.search(r'\\d+[,\\.]\\d+', rating_elem).group()\n",
    "                        \n",
    "                        # Buscar n√∫mero de rese√±as\n",
    "                        reviews_elem = parent.find(text=re.compile(r'\\d+\\s*(rese√±as?|reviews?|opiniones?)'))\n",
    "                        if reviews_elem:\n",
    "                            num_match = re.search(r'(\\d+)', reviews_elem)\n",
    "                            if num_match:\n",
    "                                num_reviews = num_match.group(1)\n",
    "                    \n",
    "                    companies.append({\n",
    "                        'company_name': company_name,\n",
    "                        'domain': domain,\n",
    "                        'company_url': company_url,\n",
    "                        'rating': rating,\n",
    "                        'num_reviews': num_reviews,\n",
    "                        'categories': 'travel_vacation'\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error procesando enlace: {e}\")\n",
    "                    continue\n",
    "        else:\n",
    "            # Procesar tarjetas normales\n",
    "            for card in company_cards:\n",
    "                try:\n",
    "                    # Buscar enlace principal\n",
    "                    link_elem = card.find('a', href=re.compile('/review/'))\n",
    "                    if not link_elem:\n",
    "                        continue\n",
    "                    \n",
    "                    company_url = link_elem.get('href', '')\n",
    "                    if not company_url.startswith('http'):\n",
    "                        company_url = f\"https://es.trustpilot.com{company_url}\"\n",
    "                    \n",
    "                    # Nombre de la empresa - m√∫ltiples estrategias\n",
    "                    company_name = None\n",
    "                    name_selectors = [\n",
    "                        ('p', {'class': re.compile('typography_heading')}),\n",
    "                        ('span', {'class': re.compile('typography_heading')}),\n",
    "                        ('h2', {}),\n",
    "                        ('h3', {}),\n",
    "                        ('p', {'class': re.compile('displayName')}),\n",
    "                        ('a', {}),  # El mismo enlace\n",
    "                    ]\n",
    "                    \n",
    "                    for tag, attrs in name_selectors:\n",
    "                        name_elem = card.find(tag, attrs)\n",
    "                        if name_elem and name_elem.text.strip():\n",
    "                            company_name = name_elem.text.strip()\n",
    "                            break\n",
    "                    \n",
    "                    if not company_name:\n",
    "                        company_name = link_elem.text.strip() or company_url.split('/')[-1]\n",
    "                    \n",
    "                    # Dominio\n",
    "                    domain = company_url.split('/')[-1] if company_url else \"N/A\"\n",
    "                    \n",
    "                    # Calificaci√≥n\n",
    "                    rating = \"N/A\"\n",
    "                    rating_patterns = [r'\\d+[,\\.]\\d+', r'\\d+\\.\\d+', r'\\d+,\\d+']\n",
    "                    for pattern in rating_patterns:\n",
    "                        rating_elem = card.find(text=re.compile(pattern))\n",
    "                        if rating_elem:\n",
    "                            match = re.search(pattern, rating_elem)\n",
    "                            if match:\n",
    "                                rating = match.group()\n",
    "                                break\n",
    "                    \n",
    "                    # N√∫mero de rese√±as\n",
    "                    num_reviews = \"0\"\n",
    "                    review_patterns = [\n",
    "                        r'(\\d+)\\s*(rese√±as?|opiniones?|reviews?)',\n",
    "                        r'(\\d+)\\s*total',\n",
    "                        r'\\((\\d+)\\)'\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in review_patterns:\n",
    "                        reviews_elem = card.find(text=re.compile(pattern, re.IGNORECASE))\n",
    "                        if reviews_elem:\n",
    "                            match = re.search(r'\\d+', reviews_elem)\n",
    "                            if match:\n",
    "                                num_reviews = match.group()\n",
    "                                break\n",
    "                    \n",
    "                    if company_name and company_name != \"N/A\":\n",
    "                        companies.append({\n",
    "                            'company_name': company_name,\n",
    "                            'domain': domain,\n",
    "                            'company_url': company_url,\n",
    "                            'rating': rating,\n",
    "                            'num_reviews': num_reviews,\n",
    "                            'categories': 'travel_vacation'\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error al procesar tarjeta: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"üìä P√°gina {page}: {len(companies)} empresas acumuladas\")\n",
    "    \n",
    "    # Eliminar duplicados\n",
    "    seen = set()\n",
    "    unique_companies = []\n",
    "    for company in companies:\n",
    "        if company['company_url'] not in seen:\n",
    "            seen.add(company['company_url'])\n",
    "            unique_companies.append(company)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total de empresas √∫nicas encontradas: {len(unique_companies)}\")\n",
    "    if unique_companies:\n",
    "        print(\"üìã Primeras 3 empresas:\")\n",
    "        for i, company in enumerate(unique_companies[:3]):\n",
    "            print(f\"   {i+1}. {company['company_name']} - {company['rating']} ({company['num_reviews']} rese√±as)\")\n",
    "    \n",
    "    return unique_companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# TEST: Ejecutar debugging para ver qu√© est√° pasando\n",
    "driver = setup_driver(headless=False)  # Usar headless=True si no quieres ver el navegador\n",
    "try:\n",
    "    # Analizar la p√°gina de categor√≠a\n",
    "    category_url = \"https://es.trustpilot.com/categories/travel_vacation\"\n",
    "    debug_page_structure(driver, category_url)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîç Intentando acceso directo a una empresa conocida...\")\n",
    "    \n",
    "    # Intentar acceder directamente a una empresa conocida\n",
    "    test_company_url = \"https://es.trustpilot.com/review/www.booking.com\"\n",
    "    driver.get(test_company_url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    if \"booking\" in driver.current_url.lower():\n",
    "        print(\"‚úÖ Acceso exitoso a p√°gina de empresa\")\n",
    "    else:\n",
    "        print(\"‚ùå Redirigido o bloqueado\")\n",
    "        print(f\"URL actual: {driver.current_url}\")\n",
    "        \n",
    "finally:\n",
    "    driver.quit()\n",
    "    print(\"\\nüîö Test completado\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para extraer rese√±as de una empresa con paginaci√≥n\n",
    "def get_reviews_from_company(driver, company_info, max_review_pages=3):\n",
    "    \"\"\"Extrae rese√±as de una empresa espec√≠fica con paginaci√≥n\"\"\"\n",
    "    reviews = []\n",
    "    subcategories = \"\"  # Variable para almacenar las subcategor√≠as\n",
    "    \n",
    "    for page in range(1, max_review_pages + 1):\n",
    "        # Construir URL con paginaci√≥n\n",
    "        if page == 1:\n",
    "            review_url = company_info['company_url']\n",
    "        else:\n",
    "            review_url = f\"{company_info['company_url']}?page={page}\"\n",
    "        \n",
    "        print(f\"   üìÑ P√°gina {page} de rese√±as: {review_url}\")\n",
    "        driver.get(review_url)\n",
    "        random_delay(2, 4)  # Delay aleatorio m√°s humano\n",
    "        \n",
    "        # Extraer subcategor√≠as del breadcrumb solo en la primera p√°gina\n",
    "        if page == 1:\n",
    "            try:\n",
    "                soup_page = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                \n",
    "                # Buscar el breadcrumb - generalmente en un nav o ol/ul con clase que contiene 'breadcrumb'\n",
    "                breadcrumb_selectors = [\n",
    "                    {'tag': 'nav', 'attrs': {'aria-label': re.compile('breadcrumb', re.I)}},\n",
    "                    {'tag': 'ol', 'attrs': {'class': re.compile('breadcrumb', re.I)}},\n",
    "                    {'tag': 'ul', 'attrs': {'class': re.compile('breadcrumb', re.I)}},\n",
    "                    {'tag': 'div', 'attrs': {'class': re.compile('breadcrumb', re.I)}},\n",
    "                    # Selector espec√≠fico de Trustpilot\n",
    "                    {'tag': 'nav', 'attrs': {'class': re.compile('styles_breadcrumbs')}},\n",
    "                ]\n",
    "                \n",
    "                breadcrumb_elem = None\n",
    "                for selector in breadcrumb_selectors:\n",
    "                    breadcrumb_elem = soup_page.find(selector['tag'], attrs=selector['attrs'])\n",
    "                    if breadcrumb_elem:\n",
    "                        break\n",
    "                \n",
    "                if breadcrumb_elem:\n",
    "                    # Extraer todos los enlaces del breadcrumb\n",
    "                    breadcrumb_links = breadcrumb_elem.find_all('a')\n",
    "                    subcategory_list = []\n",
    "                    \n",
    "                    for link in breadcrumb_links:\n",
    "                        text = link.text.strip()\n",
    "                        # Ignorar elementos gen√©ricos como \"Home\", \"Inicio\" o \"Trustpilot\"\n",
    "                        if text and text.lower() not in ['home', 'inicio', 'trustpilot']:\n",
    "                            subcategory_list.append(text)\n",
    "                    \n",
    "                    # Unir las subcategor√≠as con \" > \"\n",
    "                    subcategories = \" > \".join(subcategory_list)\n",
    "                    print(f\"   üìÅ Subcategor√≠as encontradas: {subcategories}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è No se encontraron subcategor√≠as\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error al extraer subcategor√≠as: {e}\")\n",
    "                subcategories = \"\"\n",
    "        \n",
    "        # Hacer scroll para cargar m√°s rese√±as en la p√°gina actual\n",
    "        scroll_to_load_reviews(driver, max_scrolls=3)\n",
    "        \n",
    "        # Obtener HTML actualizado\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Buscar rese√±as\n",
    "        review_cards = soup.find_all('article', class_=re.compile('paper_paper__'))\n",
    "        \n",
    "        if not review_cards:\n",
    "            print(f\"   ‚ö†Ô∏è No se encontraron rese√±as en la p√°gina {page}\")\n",
    "            break\n",
    "        \n",
    "        page_reviews = 0\n",
    "        for card in review_cards:\n",
    "            try:\n",
    "                # Nombre del cliente\n",
    "                customer_elem = card.find('span', attrs={'data-consumer-name-typography': 'true'})\n",
    "                customer_name = customer_elem.text.strip() if customer_elem else \"An√≥nimo\"\n",
    "                \n",
    "                # Fecha de la rese√±a\n",
    "                date_elem = card.find('time')\n",
    "                review_date = date_elem.get('datetime', '') if date_elem else \"\"\n",
    "                if not review_date:\n",
    "                    date_text = date_elem.text.strip() if date_elem else \"\"\n",
    "                    review_date = parse_date(date_text)\n",
    "                \n",
    "                # Puntuaci√≥n (estrellas)\n",
    "                rating_elem = card.find('div', attrs={'data-service-review-rating': True})\n",
    "                if rating_elem:\n",
    "                    rating_attr = rating_elem.get('data-service-review-rating', '0')\n",
    "                    customer_score = int(rating_attr) if rating_attr.isdigit() else 0\n",
    "                else:\n",
    "                    # Buscar alternativa\n",
    "                    star_elem = card.find('img', alt=re.compile('Valorado con'))\n",
    "                    if star_elem:\n",
    "                        alt_text = star_elem.get('alt', '')\n",
    "                        score_match = re.search(r'(\\d+)', alt_text)\n",
    "                        customer_score = int(score_match.group(1)) if score_match else 0\n",
    "                    else:\n",
    "                        customer_score = 0\n",
    "                \n",
    "                # Texto de la rese√±a\n",
    "                review_elem = card.find('p', attrs={'data-service-review-text-typography': 'true'})\n",
    "                review_text = review_elem.text.strip() if review_elem else \"\"\n",
    "                \n",
    "                # Generar ID √∫nico\n",
    "                review_id = generate_review_id(\n",
    "                    company_info['company_name'], \n",
    "                    str(review_date), \n",
    "                    customer_name, \n",
    "                    review_text\n",
    "                )\n",
    "                \n",
    "                reviews.append({\n",
    "                    'review_id': review_id,\n",
    "                    'domain': company_info['domain'],\n",
    "                    'company_name': company_info['company_name'],\n",
    "                    'categories': company_info['categories'],\n",
    "                    'subcategories': subcategories,  # Nueva columna de subcategor√≠as\n",
    "                    'company_rating': company_info['rating'],\n",
    "                    'review_date': review_date,\n",
    "                    'customer_name': customer_name,\n",
    "                    'customer_score': customer_score,\n",
    "                    'review_text': review_text,\n",
    "                    # Columnas vac√≠as para an√°lisis LLM\n",
    "                    'language': '',\n",
    "                    'sentiment': '',\n",
    "                    'emotion': '',\n",
    "                    'customer_gender': '',\n",
    "                    'main_topic': '',\n",
    "                    'keywords': '',\n",
    "                    'customer_type': '',\n",
    "                    'tourist_type': '',\n",
    "                    'group_type': '',\n",
    "                    'analyzed': False\n",
    "                })\n",
    "                page_reviews += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error al procesar rese√±a: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"   ‚úÖ P√°gina {page}: {page_reviews} rese√±as extra√≠das\")\n",
    "        \n",
    "        # Si no hay m√°s p√°ginas, salir del loop\n",
    "        if page_reviews == 0:\n",
    "            break\n",
    "            \n",
    "        # Verificar si hay bot√≥n \"Siguiente\" para continuar\n",
    "        try:\n",
    "            next_button = soup.find('a', {'aria-label': re.compile('Next|Siguiente', re.I)})\n",
    "            if not next_button or 'disabled' in str(next_button.get('class', [])):\n",
    "                print(f\"   üèÅ No hay m√°s p√°ginas de rese√±as\")\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"üìä Total: {len(reviews)} rese√±as extra√≠das de {company_info['company_name']}\")\n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n principal del scraper\n",
    "def scrape_trustpilot_travel(max_companies=10, max_review_pages_per_company=3, max_company_pages=3):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal que ejecuta todo el proceso de scraping\n",
    "    \n",
    "    Par√°metros:\n",
    "    - max_companies: N√∫mero m√°ximo de empresas a procesar\n",
    "    - max_review_pages_per_company: N√∫mero m√°ximo de p√°ginas de rese√±as por empresa\n",
    "    - max_company_pages: N√∫mero m√°ximo de p√°ginas de la categor√≠a a recorrer\n",
    "    \"\"\"\n",
    "    # URL de la categor√≠a de viajes y vacaciones\n",
    "    category_url = \"https://es.trustpilot.com/categories/travel_vacation\"\n",
    "    \n",
    "    # Inicializar driver\n",
    "    print(\"üöÄ Iniciando navegador...\")\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        # Obtener lista de empresas\n",
    "        print(f\"\\nüîç Buscando empresas en la categor√≠a de viajes...\")\n",
    "        print(f\"   ‚Ä¢ P√°ginas de categor√≠a a recorrer: {max_company_pages}\")\n",
    "        companies = get_companies_from_category(driver, category_url, max_pages=max_company_pages)\n",
    "        \n",
    "        # Limitar n√∫mero de empresas\n",
    "        companies = companies[:max_companies]\n",
    "        print(f\"\\nüìã Total de empresas a procesar: {len(companies)}\")\n",
    "        print(f\"   ‚Ä¢ P√°ginas de rese√±as por empresa: {max_review_pages_per_company}\")\n",
    "        \n",
    "        # Extraer rese√±as de cada empresa\n",
    "        all_reviews = []\n",
    "        \n",
    "        for i, company in enumerate(tqdm(companies, desc=\"Procesando empresas\")):\n",
    "            print(f\"\\n[{i+1}/{len(companies)}] üè¢ Procesando: {company['company_name']}\")\n",
    "            \n",
    "            try:\n",
    "                reviews = get_reviews_from_company(driver, company, max_review_pages=max_review_pages_per_company)\n",
    "                all_reviews.extend(reviews)\n",
    "                \n",
    "                # Pausa entre empresas para evitar bloqueos\n",
    "                random_delay(2, 4)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error al procesar {company['company_name']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Crear DataFrame\n",
    "        df_reviews = pd.DataFrame(all_reviews)\n",
    "        \n",
    "        # Guardar resultados\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"trustpilot_travel_reviews_{timestamp}.csv\"\n",
    "        df_reviews.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"\\n‚úÖ Scraping completado!\")\n",
    "        print(f\"üìä Total de rese√±as extra√≠das: {len(all_reviews)}\")\n",
    "        print(f\"üíæ Archivo guardado: {filename}\")\n",
    "        \n",
    "        # Mostrar estad√≠sticas\n",
    "        if len(df_reviews) > 0:\n",
    "            print(f\"\\nüìà Estad√≠sticas:\")\n",
    "            print(f\"   ‚Ä¢ Empresas √∫nicas: {df_reviews['company_name'].nunique()}\")\n",
    "            print(f\"   ‚Ä¢ Promedio de rese√±as por empresa: {len(df_reviews) / df_reviews['company_name'].nunique():.1f}\")\n",
    "            print(f\"   ‚Ä¢ Distribuci√≥n de puntuaciones:\")\n",
    "            score_dist = df_reviews['customer_score'].value_counts().sort_index()\n",
    "            for score, count in score_dist.items():\n",
    "                print(f\"     ‚≠ê {score}: {count} rese√±as ({count/len(df_reviews)*100:.1f}%)\")\n",
    "        \n",
    "        return df_reviews\n",
    "        \n",
    "    finally:\n",
    "        # Cerrar navegador\n",
    "        driver.quit()\n",
    "        print(\"\\nüîö Navegador cerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå SCRAPER VIEJO SIN CHECKPOINTS - NO USAR\n",
    "# (Este scraper no tiene sistema de checkpoints, usa el nuevo con checkpoints)\n",
    "# df_results = scrape_trustpilot_travel(\n",
    "#     max_companies=1500,                    # N√∫mero de empresas a procesar\n",
    "#     max_review_pages_per_company=150,     # P√°ginas de rese√±as por empresa (cada p√°gina tiene ~20 rese√±as)\n",
    "#     max_company_pages=75                 # P√°ginas de la categor√≠a a recorrer (cada p√°gina tiene ~10 empresas)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå ESTAD√çSTICAS DEL SCRAPER VIEJO - NO USAR\n",
    "# (Depende del scraper viejo, usa las funciones de consolidaci√≥n nuevas)\n",
    "# if 'df_results' in locals():\n",
    "#     print(\"üìä Resumen del dataset:\")\n",
    "#     print(f\"Total de rese√±as: {len(df_results)}\")\n",
    "#     print(f\"Empresas √∫nicas: {df_results['company_name'].nunique()}\")\n",
    "#     print(f\"\\nDistribuci√≥n de puntuaciones:\")\n",
    "#     print(df_results['customer_score'].value_counts().sort_index())\n",
    "#     \n",
    "#     # Mostrar primeras filas\n",
    "#     print(\"\\nüìã Primeras 5 rese√±as:\")\n",
    "#     display(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cargar y combinar m√∫ltiples archivos CSV\n",
    "def combine_csv_files(pattern=\"trustpilot_travel_reviews_*.csv\"):\n",
    "    \"\"\"Combina m√∫ltiples archivos CSV en un solo DataFrame\"\"\"\n",
    "    import glob\n",
    "    \n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        print(\"No se encontraron archivos CSV\")\n",
    "        return None\n",
    "    \n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file, encoding='utf-8-sig')\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Eliminar duplicados basados en review_id\n",
    "    combined_df = combined_df.drop_duplicates(subset=['review_id'])\n",
    "    \n",
    "    print(f\"Archivos combinados: {len(files)}\")\n",
    "    print(f\"Total de rese√±as √∫nicas: {len(combined_df)}\")\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistema de Checkpoints mejorado - Usando CSV\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "class ScraperCheckpointCSV:\n",
    "    \"\"\"Sistema de checkpoints usando archivos CSV para el scraper de Trustpilot\"\"\"\n",
    "    \n",
    "    def __init__(self, session_id=None):\n",
    "        # Crear ID de sesi√≥n √∫nico si no se proporciona\n",
    "        if session_id is None:\n",
    "            session_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        self.session_id = session_id\n",
    "        self.checkpoint_dir = f\"checkpoints/{session_id}\"\n",
    "        self.progress_file = f\"{self.checkpoint_dir}/progress.csv\"\n",
    "        self.errors_file = f\"{self.checkpoint_dir}/errors.csv\"\n",
    "        self.companies_file = f\"{self.checkpoint_dir}/companies_processed.csv\"\n",
    "        \n",
    "        # Crear directorio de checkpoints\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"üìÅ Sesi√≥n de checkpoint: {session_id}\")\n",
    "        print(f\"üìÅ Directorio: {self.checkpoint_dir}\")\n",
    "        \n",
    "        # Inicializar archivos si no existen\n",
    "        self._init_checkpoint_files()\n",
    "    \n",
    "    def _init_checkpoint_files(self):\n",
    "        \"\"\"Inicializa los archivos CSV de checkpoint si no existen\"\"\"\n",
    "        \n",
    "        # Archivo de progreso general\n",
    "        if not os.path.exists(self.progress_file):\n",
    "            progress_df = pd.DataFrame([{\n",
    "                'session_id': self.session_id,\n",
    "                'start_time': datetime.now().isoformat(),\n",
    "                'last_update': datetime.now().isoformat(),\n",
    "                'status': 'iniciado',\n",
    "                'companies_processed': 0,\n",
    "                'total_reviews': 0,\n",
    "                'total_csv_files': 0\n",
    "            }])\n",
    "            progress_df.to_csv(self.progress_file, index=False)\n",
    "        \n",
    "        # Archivo de empresas procesadas\n",
    "        if not os.path.exists(self.companies_file):\n",
    "            companies_df = pd.DataFrame(columns=[\n",
    "                'company_url', 'company_name', 'domain', 'processed_at', \n",
    "                'reviews_count', 'csv_filename', 'status'\n",
    "            ])\n",
    "            companies_df.to_csv(self.companies_file, index=False)\n",
    "        \n",
    "        # Archivo de errores\n",
    "        if not os.path.exists(self.errors_file):\n",
    "            errors_df = pd.DataFrame(columns=[\n",
    "                'timestamp', 'company_name', 'company_url', 'error_type', 'error_message'\n",
    "            ])\n",
    "            errors_df.to_csv(self.errors_file, index=False)\n",
    "    \n",
    "    def add_processed_company(self, company_info, reviews_count, csv_filename):\n",
    "        \"\"\"Registra una empresa como procesada en el checkpoint CSV\"\"\"\n",
    "        \n",
    "        # Actualizar archivo de empresas procesadas\n",
    "        new_row = pd.DataFrame([{\n",
    "            'company_url': company_info['company_url'],\n",
    "            'company_name': company_info['company_name'],\n",
    "            'domain': company_info['domain'],\n",
    "            'processed_at': datetime.now().isoformat(),\n",
    "            'reviews_count': reviews_count,\n",
    "            'csv_filename': csv_filename,\n",
    "            'status': 'completado'\n",
    "        }])\n",
    "        \n",
    "        # Leer archivo existente y agregar nueva fila\n",
    "        if os.path.exists(self.companies_file):\n",
    "            companies_df = pd.read_csv(self.companies_file)\n",
    "            companies_df = pd.concat([companies_df, new_row], ignore_index=True)\n",
    "        else:\n",
    "            companies_df = new_row\n",
    "        \n",
    "        companies_df.to_csv(self.companies_file, index=False)\n",
    "        \n",
    "        # Actualizar progreso general\n",
    "        self._update_progress()\n",
    "        \n",
    "        print(f\"   üìù Checkpoint actualizado: {company_info['company_name']} - {reviews_count} rese√±as\")\n",
    "    \n",
    "    def _update_progress(self):\n",
    "        \"\"\"Actualiza el archivo de progreso general\"\"\"\n",
    "        # Leer empresas procesadas\n",
    "        if os.path.exists(self.companies_file):\n",
    "            companies_df = pd.read_csv(self.companies_file)\n",
    "            total_companies = len(companies_df)\n",
    "            total_reviews = companies_df['reviews_count'].sum()\n",
    "        else:\n",
    "            total_companies = 0\n",
    "            total_reviews = 0\n",
    "        \n",
    "        # Contar archivos CSV generados\n",
    "        csv_files = glob.glob(f\"{self.checkpoint_dir}/reviews_*.csv\")\n",
    "        total_csv_files = len(csv_files)\n",
    "        \n",
    "        # Actualizar progreso\n",
    "        progress_data = {\n",
    "            'session_id': self.session_id,\n",
    "            'start_time': self._get_start_time(),\n",
    "            'last_update': datetime.now().isoformat(),\n",
    "            'status': 'en_progreso',\n",
    "            'companies_processed': total_companies,\n",
    "            'total_reviews': total_reviews,\n",
    "            'total_csv_files': total_csv_files\n",
    "        }\n",
    "        \n",
    "        progress_df = pd.DataFrame([progress_data])\n",
    "        progress_df.to_csv(self.progress_file, index=False)\n",
    "    \n",
    "    def _get_start_time(self):\n",
    "        \"\"\"Obtiene la hora de inicio de la sesi√≥n\"\"\"\n",
    "        if os.path.exists(self.progress_file):\n",
    "            progress_df = pd.read_csv(self.progress_file)\n",
    "            return progress_df.iloc[0]['start_time']\n",
    "        return datetime.now().isoformat()\n",
    "    \n",
    "    def is_company_processed(self, company_url):\n",
    "        \"\"\"Verifica si una empresa ya fue procesada\"\"\"\n",
    "        if not os.path.exists(self.companies_file):\n",
    "            return False\n",
    "        \n",
    "        companies_df = pd.read_csv(self.companies_file)\n",
    "        return company_url in companies_df['company_url'].values\n",
    "    \n",
    "    def log_error(self, company_name, company_url, error_type, error_message):\n",
    "        \"\"\"Registra un error en el checkpoint CSV\"\"\"\n",
    "        new_error = pd.DataFrame([{\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'company_name': company_name,\n",
    "            'company_url': company_url,\n",
    "            'error_type': error_type,\n",
    "            'error_message': str(error_message)\n",
    "        }])\n",
    "        \n",
    "        if os.path.exists(self.errors_file):\n",
    "            errors_df = pd.read_csv(self.errors_file)\n",
    "            errors_df = pd.concat([errors_df, new_error], ignore_index=True)\n",
    "        else:\n",
    "            errors_df = new_error\n",
    "        \n",
    "        errors_df.to_csv(self.errors_file, index=False)\n",
    "    \n",
    "    def get_processed_companies(self):\n",
    "        \"\"\"Obtiene la lista de empresas ya procesadas\"\"\"\n",
    "        if not os.path.exists(self.companies_file):\n",
    "            return []\n",
    "        \n",
    "        companies_df = pd.read_csv(self.companies_file)\n",
    "        return companies_df['company_url'].tolist()\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Obtiene un resumen del estado actual\"\"\"\n",
    "        if not os.path.exists(self.progress_file):\n",
    "            return {\"estado\": \"sin_checkpoint\"}\n",
    "        \n",
    "        progress_df = pd.read_csv(self.progress_file)\n",
    "        progress = progress_df.iloc[0]\n",
    "        \n",
    "        summary = {\n",
    "            'Sesi√≥n': self.session_id,\n",
    "            'Estado': progress['status'],\n",
    "            'Inicio': progress['start_time'],\n",
    "            '√öltima actualizaci√≥n': progress['last_update'],\n",
    "            'Empresas procesadas': progress['companies_processed'],\n",
    "            'Total de rese√±as': progress['total_reviews'],\n",
    "            'Archivos CSV': progress['total_csv_files']\n",
    "        }\n",
    "        \n",
    "        # Agregar info de errores si existen\n",
    "        if os.path.exists(self.errors_file):\n",
    "            errors_df = pd.read_csv(self.errors_file)\n",
    "            summary['Errores registrados'] = len(errors_df)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_reviews_csv(self, company_info, reviews):\n",
    "        \"\"\"Guarda las rese√±as en un archivo CSV individual\"\"\"\n",
    "        if not reviews:\n",
    "            return None\n",
    "        \n",
    "        # Crear nombre de archivo √∫nico\n",
    "        timestamp = datetime.now().strftime(\"%H%M%S\")\n",
    "        csv_filename = f\"{self.checkpoint_dir}/reviews_{company_info['domain']}_{timestamp}.csv\"\n",
    "        \n",
    "        # Convertir a DataFrame y guardar\n",
    "        df_reviews = pd.DataFrame(reviews)\n",
    "        df_reviews.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        # Solo devolver el nombre del archivo, no la ruta completa\n",
    "        return f\"reviews_{company_info['domain']}_{timestamp}.csv\"\n",
    "    \n",
    "    def consolidate_all_reviews(self):\n",
    "        \"\"\"Consolida todos los archivos CSV de rese√±as en uno solo\"\"\"\n",
    "        csv_pattern = f\"{self.checkpoint_dir}/reviews_*.csv\"\n",
    "        csv_files = glob.glob(csv_pattern)\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(\"üì≠ No se encontraron archivos CSV de rese√±as para consolidar\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üìÅ Consolidando {len(csv_files)} archivos CSV...\")\n",
    "        \n",
    "        all_reviews = []\n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                df_temp = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
    "                all_reviews.append(df_temp)\n",
    "                print(f\"   ‚úÖ {os.path.basename(csv_file)}: {len(df_temp)} rese√±as\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error leyendo {csv_file}: {e}\")\n",
    "        \n",
    "        if all_reviews:\n",
    "            # Consolidar todos los DataFrames\n",
    "            df_consolidated = pd.concat(all_reviews, ignore_index=True)\n",
    "            \n",
    "            # Eliminar duplicados por review_id\n",
    "            initial_count = len(df_consolidated)\n",
    "            df_consolidated = df_consolidated.drop_duplicates(subset=['review_id'])\n",
    "            final_count = len(df_consolidated)\n",
    "            \n",
    "            # Guardar archivo consolidado\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            consolidated_filename = f\"trustpilot_consolidated_{self.session_id}_{timestamp}.csv\"\n",
    "            df_consolidated.to_csv(consolidated_filename, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # Actualizar estado a completado\n",
    "            progress_data = {\n",
    "                'session_id': self.session_id,\n",
    "                'start_time': self._get_start_time(),\n",
    "                'last_update': datetime.now().isoformat(),\n",
    "                'status': 'completado',\n",
    "                'companies_processed': len(pd.read_csv(self.companies_file)) if os.path.exists(self.companies_file) else 0,\n",
    "                'total_reviews': final_count,\n",
    "                'total_csv_files': len(csv_files)\n",
    "            }\n",
    "            progress_df = pd.DataFrame([progress_data])\n",
    "            progress_df.to_csv(self.progress_file, index=False)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Consolidaci√≥n completada!\")\n",
    "            print(f\"üìä Archivo final: {consolidated_filename}\")\n",
    "            print(f\"üìä Total de rese√±as: {final_count}\")\n",
    "            print(f\"üìä Duplicados eliminados: {initial_count - final_count}\")\n",
    "            print(f\"üè¢ Empresas √∫nicas: {df_consolidated['company_name'].nunique()}\")\n",
    "            \n",
    "            return df_consolidated\n",
    "        \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n principal mejorada con sistema de checkpoints CSV\n",
    "def scrape_trustpilot_travel_with_checkpoint(max_companies=10, \n",
    "                                            max_review_pages_per_company=3, \n",
    "                                            max_company_pages=3,\n",
    "                                            session_id=None,\n",
    "                                            resume=True):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal con sistema de checkpoints CSV - TOTALMENTE AUTOM√ÅTICA\n",
    "    \n",
    "    Par√°metros:\n",
    "    - max_companies: N√∫mero m√°ximo de empresas a procesar\n",
    "    - max_review_pages_per_company: P√°ginas de rese√±as por empresa\n",
    "    - max_company_pages: P√°ginas de categor√≠a a recorrer\n",
    "    - session_id: ID de sesi√≥n espec√≠fico (opcional)\n",
    "    - resume: Si True, intenta resumir desde el √∫ltimo checkpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    # ü§ñ L√ìGICA AUTOM√ÅTICA DE DETECCI√ìN DE CHECKPOINTS\n",
    "    if session_id is None and resume:\n",
    "        # Buscar si hay sesiones existentes\n",
    "        if os.path.exists(\"checkpoints\"):\n",
    "            sessions = [d for d in os.listdir(\"checkpoints\") if os.path.isdir(f\"checkpoints/{d}\")]\n",
    "            if sessions:\n",
    "                # Usar la sesi√≥n m√°s reciente autom√°ticamente\n",
    "                latest_session = sorted(sessions)[-1]\n",
    "                print(f\"üîÑ MODO AUTOM√ÅTICO: Detectada sesi√≥n existente {latest_session}\")\n",
    "                print(f\"‚ôªÔ∏è Reanudando autom√°ticamente desde el √∫ltimo checkpoint...\")\n",
    "                session_id = latest_session\n",
    "            else:\n",
    "                print(f\"üÜï MODO AUTOM√ÅTICO: No hay sesiones previas, iniciando nueva sesi√≥n...\")\n",
    "        else:\n",
    "            print(f\"üÜï MODO AUTOM√ÅTICO: No hay checkpoints, iniciando desde cero...\")\n",
    "    \n",
    "    # Inicializar sistema de checkpoints CSV\n",
    "    checkpoint = ScraperCheckpointCSV(session_id=session_id)\n",
    "    \n",
    "    # URL de la categor√≠a\n",
    "    category_url = \"https://es.trustpilot.com/categories/travel_vacation\"\n",
    "    \n",
    "    # Inicializar driver\n",
    "    print(\"üöÄ Iniciando navegador...\")\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        # Obtener lista de empresas\n",
    "        print(f\"\\nüîç Buscando empresas en la categor√≠a de viajes...\")\n",
    "        companies = get_companies_from_category(driver, category_url, max_pages=max_company_pages)\n",
    "        \n",
    "        # Filtrar empresas ya procesadas si estamos resumiendo\n",
    "        if resume:\n",
    "            processed_urls = checkpoint.get_processed_companies()\n",
    "            if processed_urls:\n",
    "                print(f\"\\n‚ôªÔ∏è Filtrando empresas ya procesadas...\")\n",
    "                companies_pendientes = [c for c in companies if c['company_url'] not in processed_urls]\n",
    "                print(f\"   ‚Ä¢ Empresas totales: {len(companies)}\")\n",
    "                print(f\"   ‚Ä¢ Ya procesadas: {len(processed_urls)}\")\n",
    "                print(f\"   ‚Ä¢ Pendientes: {len(companies_pendientes)}\")\n",
    "                companies = companies_pendientes\n",
    "        \n",
    "        # Limitar n√∫mero de empresas\n",
    "        companies = companies[:max_companies]\n",
    "        print(f\"\\nüìã Total de empresas a procesar: {len(companies)}\")\n",
    "        \n",
    "        # Procesar empresas\n",
    "        for i, company in enumerate(companies):\n",
    "            print(f\"\\n[{i+1}/{len(companies)}] üè¢ Procesando: {company['company_name']}\")\n",
    "            \n",
    "            try:\n",
    "                # Extraer rese√±as\n",
    "                reviews = get_reviews_from_company(driver, company, max_review_pages=max_review_pages_per_company)\n",
    "                \n",
    "                if reviews:\n",
    "                    # Guardar rese√±as en CSV individual\n",
    "                    csv_filename = checkpoint.save_reviews_csv(company, reviews)\n",
    "                    \n",
    "                    # Actualizar checkpoint\n",
    "                    checkpoint.add_processed_company(company, len(reviews), csv_filename)\n",
    "                    print(f\"   ‚úÖ {len(reviews)} rese√±as guardadas en {csv_filename}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è No se encontraron rese√±as para {company['company_name']}\")\n",
    "                    # Registrar empresa sin rese√±as\n",
    "                    checkpoint.add_processed_company(company, 0, None)\n",
    "                \n",
    "                # Pausa entre empresas\n",
    "                random_delay(2, 4)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error al procesar {company['company_name']}: {e}\")\n",
    "                checkpoint.log_error(company['company_name'], company['company_url'], 'scraping_error', str(e))\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nüéØ Procesamiento completado!\")\n",
    "        \n",
    "        # Mostrar resumen\n",
    "        summary = checkpoint.get_summary()\n",
    "        print(\"\\nüìà Resumen de la sesi√≥n:\")\n",
    "        for key, value in summary.items():\n",
    "            print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "            \n",
    "        return checkpoint\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Proceso interrumpido por el usuario\")\n",
    "        print(\"üíæ El progreso se ha guardado. Puedes resumir la ejecuci√≥n m√°s tarde.\")\n",
    "        print(f\"üíæ ID de sesi√≥n: {checkpoint.session_id}\")\n",
    "        return checkpoint\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error general: {e}\")\n",
    "        checkpoint.log_error('SISTEMA', 'GENERAL', 'sistema_error', str(e))\n",
    "        return checkpoint\n",
    "        \n",
    "    finally:\n",
    "        # Cerrar navegador\n",
    "        driver.quit()\n",
    "        print(\"\\nüîö Navegador cerrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de utilidad para gesti√≥n de checkpoints CSV\n",
    "\n",
    "def list_checkpoint_sessions():\n",
    "    \"\"\"Lista todas las sesiones de checkpoint disponibles\"\"\"\n",
    "    if not os.path.exists(\"checkpoints\"):\n",
    "        print(\"üì≠ No hay sesiones de checkpoint\")\n",
    "        return []\n",
    "    \n",
    "    sessions = [d for d in os.listdir(\"checkpoints\") if os.path.isdir(f\"checkpoints/{d}\")]\n",
    "    \n",
    "    if not sessions:\n",
    "        print(\"üì≠ No hay sesiones de checkpoint\")\n",
    "        return []\n",
    "    \n",
    "    print(\"üìÅ Sesiones disponibles:\")\n",
    "    session_data = []\n",
    "    \n",
    "    for session_id in sorted(sessions):\n",
    "        try:\n",
    "            checkpoint = ScraperCheckpointCSV(session_id=session_id)\n",
    "            summary = checkpoint.get_summary()\n",
    "            session_data.append({\n",
    "                'session_id': session_id,\n",
    "                'status': summary.get('Estado', 'desconocido'),\n",
    "                'companies': summary.get('Empresas procesadas', 0),\n",
    "                'reviews': summary.get('Total de rese√±as', 0),\n",
    "                'last_update': summary.get('√öltima actualizaci√≥n', 'N/A')\n",
    "            })\n",
    "            print(f\"   üîπ {session_id}: {summary.get('Estado')} - {summary.get('Empresas procesadas', 0)} empresas, {summary.get('Total de rese√±as', 0)} rese√±as\")\n",
    "        except:\n",
    "            print(f\"   ‚ùå {session_id}: Error al leer sesi√≥n\")\n",
    "    \n",
    "    return session_data\n",
    "\n",
    "def view_checkpoint_status(session_id=None):\n",
    "    \"\"\"Muestra el estado de una sesi√≥n espec√≠fica o la m√°s reciente\"\"\"\n",
    "    if session_id is None:\n",
    "        # Buscar la sesi√≥n m√°s reciente\n",
    "        sessions = list_checkpoint_sessions()\n",
    "        if not sessions:\n",
    "            return\n",
    "        session_id = sessions[-1]['session_id']\n",
    "        print(f\"\\nüìä Mostrando sesi√≥n m√°s reciente: {session_id}\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = ScraperCheckpointCSV(session_id=session_id)\n",
    "        summary = checkpoint.get_summary()\n",
    "        \n",
    "        print(\"\\nüìä ESTADO DEL CHECKPOINT\")\n",
    "        print(\"=\" * 50)\n",
    "        for key, value in summary.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        # Mostrar empresas procesadas\n",
    "        if os.path.exists(checkpoint.companies_file):\n",
    "            companies_df = pd.read_csv(checkpoint.companies_file)\n",
    "            if len(companies_df) > 0:\n",
    "                print(f\"\\nüìã √öltimas 5 empresas procesadas:\")\n",
    "                for _, company in companies_df.tail(5).iterrows():\n",
    "                    status_icon = \"‚úÖ\" if company['status'] == 'completado' else \"‚ö†Ô∏è\"\n",
    "                    print(f\"   {status_icon} {company['company_name']}: {company['reviews_count']} rese√±as\")\n",
    "        \n",
    "        # Mostrar errores si existen\n",
    "        if os.path.exists(checkpoint.errors_file):\n",
    "            errors_df = pd.read_csv(checkpoint.errors_file)\n",
    "            if len(errors_df) > 0:\n",
    "                print(f\"\\n‚ö†Ô∏è √öltimos 3 errores:\")\n",
    "                for _, error in errors_df.tail(3).iterrows():\n",
    "                    print(f\"   ‚Ä¢ {error['company_name']}: {error['error_message'][:50]}...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al leer sesi√≥n {session_id}: {e}\")\n",
    "\n",
    "def consolidate_session_reviews(session_id=None):\n",
    "    \"\"\"Consolida todas las rese√±as de una sesi√≥n espec√≠fica\"\"\"\n",
    "    if session_id is None:\n",
    "        # Usar la sesi√≥n m√°s reciente\n",
    "        sessions = list_checkpoint_sessions()\n",
    "        if not sessions:\n",
    "            print(\"üì≠ No hay sesiones disponibles\")\n",
    "            return None\n",
    "        session_id = sessions[-1]['session_id']\n",
    "        print(f\"üîç Usando sesi√≥n m√°s reciente: {session_id}\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = ScraperCheckpointCSV(session_id=session_id)\n",
    "        df_consolidated = checkpoint.consolidate_all_reviews()\n",
    "        return df_consolidated\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al consolidar sesi√≥n {session_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def resume_session(session_id, max_companies=None, max_review_pages_per_company=3, max_company_pages=3):\n",
    "    \"\"\"Reanuda una sesi√≥n espec√≠fica de scraping\"\"\"\n",
    "    print(f\"‚ôªÔ∏è Reanudando sesi√≥n: {session_id}\")\n",
    "    \n",
    "    return scrape_trustpilot_travel_with_checkpoint(\n",
    "        max_companies=max_companies or 9999,  # Si no se especifica, procesar todas las pendientes\n",
    "        max_review_pages_per_company=max_review_pages_per_company,\n",
    "        max_company_pages=max_company_pages,\n",
    "        session_id=session_id,\n",
    "        resume=True\n",
    "    )\n",
    "\n",
    "def export_session_errors(session_id=None):\n",
    "    \"\"\"Exporta los errores de una sesi√≥n espec√≠fica\"\"\"\n",
    "    if session_id is None:\n",
    "        sessions = list_checkpoint_sessions()\n",
    "        if not sessions:\n",
    "            print(\"üì≠ No hay sesiones disponibles\")\n",
    "            return\n",
    "        session_id = sessions[-1]['session_id']\n",
    "    \n",
    "    try:\n",
    "        checkpoint = ScraperCheckpointCSV(session_id=session_id)\n",
    "        \n",
    "        if not os.path.exists(checkpoint.errors_file):\n",
    "            print(\"‚úÖ No hay errores registrados en esta sesi√≥n\")\n",
    "            return\n",
    "        \n",
    "        errors_df = pd.read_csv(checkpoint.errors_file)\n",
    "        if len(errors_df) == 0:\n",
    "            print(\"‚úÖ No hay errores registrados en esta sesi√≥n\")\n",
    "            return\n",
    "        \n",
    "        # Guardar copia de errores con timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"errores_{session_id}_{timestamp}.csv\"\n",
    "        errors_df.to_csv(output_filename, index=False)\n",
    "        \n",
    "        print(f\"üìã Errores exportados: {output_filename}\")\n",
    "        print(f\"   Total de errores: {len(errors_df)}\")\n",
    "        \n",
    "        # Mostrar resumen de tipos de errores\n",
    "        print(f\"\\nüìä Tipos de errores:\")\n",
    "        error_summary = errors_df['error_type'].value_counts()\n",
    "        for error_type, count in error_summary.items():\n",
    "            print(f\"   ‚Ä¢ {error_type}: {count}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al exportar errores de sesi√≥n {session_id}: {e}\")\n",
    "\n",
    "def delete_session(session_id, confirm=True):\n",
    "    \"\"\"Elimina una sesi√≥n completa de checkpoint\"\"\"\n",
    "    if confirm:\n",
    "        response = input(f\"‚ö†Ô∏è ¬øEst√°s seguro de que quieres eliminar la sesi√≥n {session_id}? (s/n): \")\n",
    "        if response.lower() != 's':\n",
    "            print(\"‚ùå Operaci√≥n cancelada\")\n",
    "            return\n",
    "    \n",
    "    import shutil\n",
    "    session_dir = f\"checkpoints/{session_id}\"\n",
    "    \n",
    "    if os.path.exists(session_dir):\n",
    "        shutil.rmtree(session_dir)\n",
    "        print(f\"üóëÔ∏è Sesi√≥n {session_id} eliminada\")\n",
    "    else:\n",
    "        print(f\"‚ùå La sesi√≥n {session_id} no existe\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ü§ñ MODO AUTOM√ÅTICO - Sistema de Checkpoints\n",
    "\n",
    "### ‚ö° EJECUCI√ìN SIMPLE (3 pasos):\n",
    "\n",
    "1. **üöÄ EJECUTAR:** Celda siguiente - Inicia o reanuda autom√°ticamente\n",
    "2. **üìä MONITOREAR:** Ver progreso en tiempo real \n",
    "3. **üìÅ CONSOLIDAR:** Unir todos los CSV al final\n",
    "\n",
    "### üß† COMPORTAMIENTO INTELIGENTE:\n",
    "\n",
    "- ‚úÖ **Si NO hay checkpoints**: Empieza desde cero\n",
    "- ‚úÖ **Si HAY checkpoints**: Reanuda autom√°ticamente desde el √∫ltimo\n",
    "- ‚úÖ **Si se interrumpe**: Guarda progreso cada empresa procesada\n",
    "- ‚úÖ **Al finalizar**: Consolida todo en un CSV final\n",
    "\n",
    "### üìã Solo necesitas ejecutar las celdas descomentadas en orden ‚¨áÔ∏è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ PASO 1: EJECUTAR SCRAPER CON CHECKPOINTS AUTOM√ÅTICOS\n",
    "# Descomenta las l√≠neas de abajo para iniciar (quita los # del inicio):\n",
    "\n",
    "checkpoint = scrape_trustpilot_travel_with_checkpoint(\n",
    "    max_companies=1500,                    # Procesar hasta 1500 empresas\n",
    "    max_review_pages_per_company=150,      # 150 p√°ginas de rese√±as por empresa\n",
    "    max_company_pages=75,                  # Buscar en 75 p√°ginas de la categor√≠a\n",
    "    session_id=None,                       # Se crear√° ID autom√°tico si no hay sesiones\n",
    "    resume=True                            # AUTOM√ÅTICO: reanuda si hay checkpoint\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä PASO 2: MONITOREO AUTOM√ÅTICO - Ver progreso en tiempo real\n",
    "# Descomenta para ver el estado actual (quita los # del inicio):\n",
    "\n",
    "list_checkpoint_sessions()  # Ver todas las sesiones\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "view_checkpoint_status()    # Ver detalles de la sesi√≥n m√°s reciente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç OPCIONAL: Ver estado de una sesi√≥n espec√≠fica\n",
    "# view_checkpoint_status(\"20241201_143022\")  # Para sesi√≥n espec√≠fica (descomenta si necesitas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ôªÔ∏è SOLO SI NECESITAS: Reanudar una sesi√≥n espec√≠fica manualmente\n",
    "# (El modo autom√°tico ya hace esto, pero si quieres forzar una sesi√≥n espec√≠fica:)\n",
    "# checkpoint = resume_session(\n",
    "#     session_id=\"20241201_143022\",         # ID de la sesi√≥n a reanudar\n",
    "#     max_companies=100,                    # Continuar hasta 100 empresas en total\n",
    "#     max_review_pages_per_company=3\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ PASO 3: CONSOLIDACI√ìN AUTOM√ÅTICA - Unir todos los CSV en un archivo final\n",
    "# Descomenta para consolidar autom√°ticamente (quita los # del inicio):\n",
    "\n",
    "df_final = consolidate_session_reviews()  # Sesi√≥n m√°s reciente autom√°ticamente\n",
    "\n",
    "# # Mostrar estad√≠sticas del resultado final:\n",
    "if df_final is not None:\n",
    "    print(f\"\\nüéâ CONSOLIDACI√ìN COMPLETADA!\")\n",
    "    print(f\"üìä Total de rese√±as consolidadas: {len(df_final):,}\")\n",
    "    print(f\"üè¢ Empresas √∫nicas: {df_final['company_name'].nunique()}\")\n",
    "    print(f\"üìà Promedio de rese√±as por empresa: {len(df_final) / df_final['company_name'].nunique():.1f}\")\n",
    "    print(\"\\n‚≠ê Distribuci√≥n de puntuaciones:\")\n",
    "    score_dist = df_final['customer_score'].value_counts().sort_index()\n",
    "    for score, count in score_dist.items():\n",
    "        percentage = (count / len(df_final)) * 100\n",
    "        print(f\"   {score} estrellas: {count:,} rese√±as ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay datos para consolidar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã OPCIONAL: Gesti√≥n de errores y limpieza (descomenta solo si necesitas)\n",
    "# export_session_errors()                  # Exportar errores de la sesi√≥n m√°s reciente\n",
    "# export_session_errors(\"20241201_143022\") # Exportar errores de sesi√≥n espec√≠fica\n",
    "# delete_session(\"20241201_143022\")        # Eliminar una sesi√≥n completa (con confirmaci√≥n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Funci√≥n para filtrar rese√±as por puntuaci√≥n o fecha\n",
    "def filter_reviews(df, min_score=None, max_score=None, start_date=None, end_date=None):\n",
    "    \"\"\"Filtra rese√±as seg√∫n criterios espec√≠ficos\"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    if min_score is not None:\n",
    "        filtered_df = filtered_df[filtered_df['customer_score'] >= min_score]\n",
    "    \n",
    "    if max_score is not None:\n",
    "        filtered_df = filtered_df[filtered_df['customer_score'] <= max_score]\n",
    "    \n",
    "    if start_date is not None:\n",
    "        filtered_df['review_date'] = pd.to_datetime(filtered_df['review_date'])\n",
    "        filtered_df = filtered_df[filtered_df['review_date'] >= start_date]\n",
    "    \n",
    "    if end_date is not None:\n",
    "        filtered_df['review_date'] = pd.to_datetime(filtered_df['review_date'])\n",
    "        filtered_df = filtered_df[filtered_df['review_date'] <= end_date]\n",
    "    \n",
    "    print(f\"Rese√±as filtradas: {len(filtered_df)} de {len(df)}\")\n",
    "    return filtered_df\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# df_filtered = filter_reviews(df_results, min_score=4, start_date='2024-01-01')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
